% !Mode:: "TeX:UTF-8"

@Inbook{Wu2016,
author="Wu, Yunfang
and Li, Wei",
editor="Lin, Chin-Yew
and Xue, Nianwen
and Zhao, Dongyan
and Huang, Xuanjing
and Feng, Yansong",
title="Overview of the NLPCC-ICCPOL 2016 Shared Task: Chinese Word Similarity Measurement",
bookTitle="Natural Language Understanding and Intelligent Applications: 5th CCF Conference on Natural Language Processing and Chinese Computing, NLPCC 2016, and 24th International Conference on Computer Processing of Oriental Languages, ICCPOL 2016, Kunming, China, December 2--6, 2016, Proceedings",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="828--839",
isbn="978-3-319-50496-4",
doi="10.1007/978-3-319-50496-4_75",
url="http://dx.doi.org/10.1007/978-3-319-50496-4_75"
}

@article{Hochreiter1997,
author = { Sepp Hochreiter  and  Jürgen Schmidhuber },
title = {Long Short-Term Memory},
journal = {Neural Computation},
volume = {9},
number = {8},
pages = {1735-1780},
year = {1997},
doi = {10.1162/neco.1997.9.8.1735},

URL = { 
        http://dx.doi.org/10.1162/neco.1997.9.8.1735
    
},
eprint = { 
        http://dx.doi.org/10.1162/neco.1997.9.8.1735
    
}
,
    abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}

@article{Gers2000,
author = { Felix A. Gers  and  Jürgen Schmidhuber  and  Fred Cummins },
title = {Learning to Forget: Continual Prediction with LSTM},
journal = {Neural Computation},
volume = {12},
number = {10},
pages = {2451-2471},
year = {2000},
doi = {10.1162/089976600300015015},

URL = { 
        http://dx.doi.org/10.1162/089976600300015015
    
},
eprint = { 
        http://dx.doi.org/10.1162/089976600300015015
    
}
,
    abstract = { Long short-term memory (LSTM; Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive “forget gate” that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way. }
}

@inproceedings{Sak2014,
	author    = {Hasim Sak and
	Andrew W. Senior and
	Fran{\c{c}}oise Beaufays},
	title     = {Long short-term memory recurrent neural network architectures for
	large scale acoustic modeling},
	booktitle = {{INTERSPEECH} 2014, 15th Annual Conference of the International Speech
	Communication Association, Singapore, September 14-18, 2014},
	pages     = {338--342},
	year      = {2014},
	crossref  = {DBLP:conf/interspeech/2014},
	url       = {http://www.isca-speech.org/archive/interspeech_2014/i14_0338.html},
	timestamp = {Mon, 30 Jan 2017 12:13:12 +0100},
	biburl    = {http://dblp.uni-trier.de/rec/bib/conf/interspeech/SakSB14},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@proceedings{DBLP:conf/interspeech/2014,
	editor    = {Haizhou Li and
	Helen M. Meng and
	Bin Ma and
	Engsiong Chng and
	Lei Xie},
	title     = {{INTERSPEECH} 2014, 15th Annual Conference of the International Speech
	Communication Association, Singapore, September 14-18, 2014},
	publisher = {{ISCA}},
	year      = {2014},
	url       = {http://www.isca-speech.org/archive/interspeech_2014},
	timestamp = {Mon, 30 Jan 2017 12:13:12 +0100},
	biburl    = {http://dblp.uni-trier.de/rec/bib/conf/interspeech/2014},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Duchi2011,
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	journal = {J. Mach. Learn. Res.},
	issue_date = {2/1/2011},
	volume = {12},
	month = 7,
	year = {2011},
	issn = {1532-4435},
	pages = {2121--2159},
	numpages = {39},
	url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
	acmid = {2021068},
	publisher = {JMLR.org},
}

@article{Zeiler2012,
	author    = {Matthew D. Zeiler},
	title     = {{ADADELTA:} An Adaptive Learning Rate Method},
	journal   = {CoRR},
	volume    = {abs/1212.5701},
	year      = {2012},
	url       = {http://arxiv.org/abs/1212.5701},
	timestamp = {Wed, 02 Jan 2013 09:49:04 +0100},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1212-5701},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Kingma2014,
	author    = {Diederik P. Kingma and
	Jimmy Ba},
	title     = {Adam: {A} Method for Stochastic Optimization},
	journal   = {CoRR},
	volume    = {abs/1412.6980},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.6980},
	timestamp = {Thu, 01 Jan 2015 19:51:08 +0100},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KingmaB14},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Ruder2016,
	author    = {Sebastian Ruder},
	title     = {An overview of gradient descent optimization algorithms},
	journal   = {CoRR},
	volume    = {abs/1609.04747},
	year      = {2016},
	url       = {http://arxiv.org/abs/1609.04747},
	timestamp = {Mon, 03 Oct 2016 17:51:10 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Ruder16},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}